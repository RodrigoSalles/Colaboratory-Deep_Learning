{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning + Colaboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### “Colaboratory is a Google research project created to help disseminate machine learning education and research. It's a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Utilizando a ferramenta Colaboratory para criar projetos de machine learning.</i>\n",
    "\n",
    "<b>Dados utilizados:</b> \n",
    "\n",
    "Dataset of handwritten digits - MNIST (\"Modified National Institute of Standards and Technology\").\n",
    "\n",
    "Cada imagem, em tons de cinza, possui definição de 28x28 pixels, totalizando 784 pixels. Cada pixels é representado por um valor, que varia entre 0 e 255, e indica o quão claro ou escuro é esse pixel.\n",
    "\n",
    "Os dados utilizados podem ser encontrados <a href = 'https://www.kaggle.com/c/digit-recognizer'> aqui</a>.\n",
    "                                      \n",
    "<img src=\"digit.png\" height=\"212\" width=\"400\">                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O problema!\n",
    "\n",
    "Você já tentou treinar uma rede neural artificial (RNA) um pouco mais complexa (no contexto deep learning, convolutional neural network) no seu notebook? Eu já. E essa experiência não foi muito agradável. Horas a esperar o resultado, para constatar que poderia ter modificado um parâmetro, ou mudado alguma coisa. Ao tentar obter melhores resultados a rede fica mais complexa, e o tempo de treinamento torna o projeto inviável. \n",
    "\n",
    "Foi esse o cenário que me deparei ao tentar utilizar uma RNA um pouco mais elaborada para realizar a identificação de dígitos manuscritos. \n",
    "\n",
    "Iniciei uma pesquisa para tentar resolver meu problema. A melhor opção que encontrei foi a ferramenta disponibilizada pelo Google, chamada Colaboratory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mas o que é o Colaboratory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como explica o texto de introdução, Colaboratory é um projeto criado para auxiliar o estudo e pesquisa em aprendizado de máquinas, no qual você tem acesso a uma GPU de 80K! E o melhor....de graça.\n",
    "\n",
    "O projeto é baseado no Jupyter Notebook, rodando completamente em nuvem, de simples utilização, mas com resultados animadores. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como foi falado, esse pequeno tutorial terá como objetivo auxiliar no desenvolvimento de um projeto utilizando a ferramenta Colaboratory. Os dados utilizados são bem conhecidos e exigem pouco pré-processamento. O resultado poderia ser melhorado? Sim. Mas o objetivo principal aqui é mostrar o funcionamento da ferramenta Colab, de forma que você possa aplicá-la em seus projetos pessoais. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ao trabalho!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao pesquisar por Google Colaboratory você será direcionado à página inicial do projeto:\n",
    "\n",
    "<img src=\"fig1.png\" height=\"150\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao clicar em “GO to Colaboratory” você será direcionado para a página de acesso do Gmail.  Ao entrar com sua senha, você já estará no Colaboratory.\n",
    "\n",
    "<img src=\"fig2.png\" height=\"250\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, o processo é bem simples: File > New Python 3 notebook.\n",
    "\n",
    "<img src=\"fig3.png\" height=\"250\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E voilà...temos um novo notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig4.png\" height=\"600\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a criação do notebook você poderá fazer as modificações iniciais de costume como alterar nomes, modificar aparência, etc.\n",
    "\n",
    "Vamos agora acrescentar a GPU ao nosso notebook. O processo é simples: Runtime > Change runtime type . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig5.png\" height=\"300\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em “Notebook settings” você mudará o “Hardware accelerator”, de “None”, para “GPU”. Após realizar essas operações clique em “Save”, e já temos uma ótima GPU para o nosso trabalho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig6.png\" height=\"400\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora temos que realizar uma pequena operação que vai nos salvar de muita dor de cabeça. Não se preocupe, é rápido.\n",
    "\n",
    "Se você está trabalhando com data science, com certeza você precisará de um dataset. Como todo o nosso trabalho está em nuvem, teremos que perder algum tempo para fazer o upload desses dados.  \n",
    "\n",
    "Esse processo pode ser feito com o auxílio do código fornecido nas páginas iniciais do Colaboratory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao rodar esse código poderemos escolher os arquivos, fazer o upload e trabalhar com os dados. O problema é que teríamos que refazer esse processo toda vez que o notebook fosse reiniciado. E você sabe que subir dados pode ser um processo demorado. E não temos muito tempo para perder. \n",
    "\n",
    "Em minhas pesquisas descobri uma melhor forma de trabalho. Na realidade eu fiz um pequeno Frankenstein: copiando um pedaço de código aqui...outro ali....e assim foi. Vamos a isso.\n",
    "\n",
    "Como nosso projeto no Colab fica guardado no Google Drive, vamos criar uma pasta no nosso Drive e vamos guardar nessa pasta nosso notebook e todos os nossos dados. Vamos criar uma “conexão” entre nosso notebook e nossos dados, que estarão disponíveis para todo o sempre depois de feito o upload.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodando o seguinte código, você instalará o “FUSE wrapper”, necessário para os próximos passos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando o Drive FUSE wrapper.\n",
    "# https://github.com/astrada/google-drive-ocamlfuse\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir você deverá rodar o seguinte código que criará um “token” de conexão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate auth tokens for Colab\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig8.png\" height=\"200\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig7.png\" height=\"400\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao clicar no link, você será direcionado ao seu e-mail, e ao entrar com a senha, você será direcionado a uma página que pede acesso ao seu Drive. Pode aceitar sem medo.\n",
    "\n",
    "Ao clicar em “Permitir”, o sistema criará uma senha de acesso:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig9.png\" height=\"400\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você vai copiar esse código e colar lá no seu do notebook.\n",
    "\n",
    "Agora você fará o mesmo processo com o seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate creds for the Drive FUSE library.\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao permitir o acesso, novo token será criado, e ao entrar com esse passe você terá a seguinte saída: <b> Access token retrieved correctly.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim esses últimos comandos nos permitem acesso ao nosso drive.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory and mount Google Drive using that directory.\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim! A conexão entre o seu programa e seus arquivos já foi estabelecida. Você já pode trabalhar. O processo parece complicado, mas não é. O código já está todo pronto, você não precisa modificar nada, e tudo pode ser feito em menos de 2 minutos. \n",
    "\n",
    "Lá no Google Drive eu vou criar uma nova pasta, e vou fazer o upload de todos dos dados que eu preciso.\n",
    "\n",
    "Bom... a minha pasta chama-se <b>Digit_recognition</b>, e nessa pasta eu guardei os arquivos “<b>train</b>” e “<b>test</b>”.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig10.png\" height=\"200\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posso verificar quais arquivos estão presentes em meu drive com o comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Files in my drive:\")\n",
    "!ls drive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig11.png\" height=\"500\" width=\"600\">\n",
    "\n",
    "Lá está nossa pasta! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, com tudo pronto, todas as conexões estabelecidas, e todos os dados disponíveis....vamos ao que interessa... <i>let’s have some fun!!</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabalhando com os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como foi falado anteriormente, o objetivo aqui é mostrar como utilizar o Colaboratory. Não vou entrar em maiores detalhes sobre técnicas de machine learning. Isso fica para uma próxima oportunidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As principais bibliotecas utilizadas em data science já estão presentes no Colab. O restante você poderá instalar de forma muito fácil utilizando “pip” do Python. “A única diferença é que se deve acrescentar o símbolo <b>\"!\"</b> antes de cada comando”. \n",
    "\n",
    "Vamos agora carregar os dados que estão na pasta Digit_recognition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('drive/Digit_recognition/train.csv', encoding = 'utf8') \n",
    "test = pd.read_csv('drive/Digit_recognition/test.csv', encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos observar nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotar alguns valores do banco de dados:\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[8], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[9], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig12.png\" height=\"300\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso classificador será desenvolvido com o auxílio do Keras, que é uma high-level neural network API. É Uma forma mais fácil de desenvolver protótipos em deep learning, CNNs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig13.png\" height=\"300\" width=\"400\">\n",
    "\n",
    "Se você não conhece o Keras vale a pena conferir <a href =  https://keras.io/> aqui </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando o comando <b>”!pip”</b> instalamos o Keras, com backend tensorflow. Se você prefere trabalhar com o theano, a troca é simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig14.png\" height=\"500\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso banco de dados é composto por imagens de caracteres manuscritos de 28x28 pixels, em tons de cinza. Essas imagens foram rearranjadas em vetores com 784 elementos. \n",
    "\n",
    "Para adequar esses dados ao nosso classificador vamos realizar alguns processos básicos: normalizar, codificar e dividir o dataset em conjunto de treino e teste.\n",
    "\n",
    "Uma observação: como esses dados serão utilizados em uma competição (kaggle), nós vamos dividir o conjunto de treino em treino e teste. O conjunto de testes <i>\"pra valer\"</i> será deixado para criar o arquivo final de submissão. Você não sabe como funcionam essas competições? Olhe <a href = https://www.kaggle.com/> aqui</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando os dados:\n",
    "X_train = (train.ix[:,1:].values).astype('float32') \n",
    "y_train = train.ix[:,0].values.astype('int32') \n",
    "X_test = test.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando o banco de dados em treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_train, y_train, test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando os dados: de 0-255 para 0-1\n",
    "train_X = train_X / 255\n",
    "test_X = test_X / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificando o y:\n",
    "from keras.utils import np_utils\n",
    "train_y = np_utils.to_categorical(train_y)\n",
    "test_y = np_utils.to_categorical(test_y)\n",
    "num_classes = test_y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando nossa rede neural convolucional (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados prontos, vamos iniciar o modelo de nossa rede neural convolucional (CNN). Vamos começar com uma rede mais simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projetando a CNN:\n",
    "import numpy as np\n",
    "seed = 5\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 1:\n",
    "num_pixels = train_X.shape[1]\n",
    "def model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos treinar e avaliar esse modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodando modelo_1:\n",
    "model = model_1()\n",
    "model.fit(train_X, train_y, validation_data=(test_X, test_y), epochs=10, batch_size=200, verbose=2)\n",
    "# Avaliando o modelo:\n",
    "scores = model.evaluate(test_X, test_y, verbose=0)\n",
    "print(\"Erro: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No meu PC, que já está cansado, esse modelo demorou um pouco. Aqui a história é outra:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig15.png\" height=\"400\" width=\"500\">\n",
    "\n",
    "Em poucos segundo o processo termina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom...já temos um erro pequeno, em comparação com outros classificadores. Vamos implementar uma rede um pouco mais elaborada e ver o resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparando os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.reshape(train_X.shape[0], 1, 28, 28).astype('float32')\n",
    "test_X = test_X.reshape(test_X.shape[0], 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodando modelo_2:\n",
    "model = model_2()\n",
    "model.fit(train_X, train_y, validation_data=(test_X, test_y), epochs=10, batch_size=200, verbose=2)\n",
    "# Avaliando o modelo:\n",
    "scores = model.evaluate(test_X, test_y, verbose=0)\n",
    "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando e avaliando esse modelo, teremos o seguinte resultado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig16.png\" height=\"400\" width=\"500\">\n",
    "\n",
    "Pode-se observar que o erro cai de 2,27% para 1,35% das classificações. Muito bom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para terminar, vamos tentar uma CNN mais complexa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodando modelo_3:\n",
    "model = model_3()\n",
    "model.fit(train_X, train_y, validation_data=(test_X, test_y), epochs=10, batch_size=200)\n",
    "# Avaliando o modelo:\n",
    "scores = model.evaluate(test_X, test_y, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig17.png\" height=\"600\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como é possível verificar, os resultados foram obtidos em poucos segundos.  Todo esse processo demoraria uma eternidade em meu PC, e no Colab a coisa terminou em segundos. Muito bom!\n",
    "\n",
    "O erro final foi de 1,29%, e esse ultimo modelo será usado para criar o arquivo csv final. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o arquivo de submissão "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodando esse pequeno código a seguir, nós criaremos um arquivo .csv que será armazenado na mesma pasta de nosso projeto, \n",
    "no Google drive. Agora é só baixar esse arquivo e submeter à competição. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test/255\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_3()\n",
    "predictions = model.predict_classes(X_test, verbose=0)\n",
    "\n",
    "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
    "                         \"Label\": predictions})\n",
    "submissions.to_csv(\"drive/submissions_27_02.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse pequeno tutorial mostrou como configurar e trabalhar com o google Colaboratory. Demonstrou como fazer a conexão entre o Colaboratory e o Google Drive, e como instalar algumas importantes ferramentas. \n",
    "\n",
    "Espero ter ajudado de alguma forma. Para mim essa ferramenta foi de grande valia. O projeto poderia ser melhorado, muitos conceitos e técnicas poderiam ser aplicados para melhorar o resultado, mas o tutorial perderia um pouco o foco, e ficaria muito extenso. \n",
    "\n",
    "Se você não compreendeu algum ponto, ou gostaria de acrescentar alguma coisa, Fique a vontade para me escrever. Até a próxima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "\n",
    "* https://www.kaggle.com/serigne\n",
    "\n",
    "* https://paulovasconcellos.com.br/como-criar-seu-primeiro-projeto-de-data-science-parte-2-de-2-cb9a2fe05eff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
